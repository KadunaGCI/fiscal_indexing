{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip,os,tarfile,sys\n",
    "sys.path.append(os.pardir+'/src')\n",
    "from settings import *\n",
    "from boto3.session import Session\n",
    "import datetime\n",
    "import traceback\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import xml.etree.ElementTree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# S3から記事データをダウンロードする関数\n",
    "def downloadFile(bucket, tag, target_day):\n",
    "    # バケットから指定されたタグと日付に該当するオブジェクトを取得\n",
    "    objects = bucket.objects.all().filter(Prefix=tag+target_day)\n",
    "\n",
    "    for object in objects:\n",
    "        # データを格納するパスを生成\n",
    "        path = os.path.join(DATA_DIR,tag+target_day)\n",
    "        # ダウンロードを実施\n",
    "        bucket.download_file(object.key, path)\n",
    "\n",
    "# startで指定された日付からspan日分のファイル名配列を生成\n",
    "def makeDateList(start, span):\n",
    "    dateList = []\n",
    "\n",
    "    for i in range(int(span)):\n",
    "        dateList.append('EID42168_' + start.strftime(\"%Y%m%d\") + '.xml.gz')\n",
    "        start = start + datetime.timedelta(days=1)\n",
    "\n",
    "    return dateList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# コマンドライン引数からダウンロードを開始する日付と範囲を取得\n",
    "start_date = '20140101'\n",
    "span = '365'\n",
    "start = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "\n",
    "# S3へ接続\n",
    "session = Session(aws_access_key_id=AWS_ACCESS_KEY_ID,aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "s3 = session.resource('s3')\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "\n",
    "# tagと日付リストを設定\n",
    "tag = \"EID42168_\"\n",
    "dateList = makeDateList(start, span)\n",
    "\n",
    "for date in dateList:\n",
    "    time = datetime.datetime.now()\n",
    "    date = date[9:]\n",
    "    try:\n",
    "        downloadFile(bucket, tag, date)\n",
    "        print(date + ' was done ' + str(datetime.datetime.now()-time))\n",
    "    except Exception as e:\n",
    "        print('error! ' + date)\n",
    "        print(logging.error(traceback.format_exc()))\n",
    "\n",
    "print('it taked ' + str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 片山さんのプログラムによる抽出 csv1へ格納\n",
    "import gzip,os,tarfile,sys\n",
    "from settings import *\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from boto3.session import Session\n",
    "import xml.etree.ElementTree as et\n",
    "import datetime\n",
    "\n",
    "\n",
    "# xml.gzファイルを解凍、xmlの構造を解析しCSVファイルを作成する関数\n",
    "def convertToCSV(file_name):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # 引数として渡されたtar.gzファイルを解凍し、オープン\n",
    "    f = gzip.open(os.path.join(DATA_DIR,'original_Data',file_name), 'r')\n",
    "\n",
    "    # CSVファイルのカラムに対応する配列を初期化\n",
    "    ids = []\n",
    "    headlines = []\n",
    "    timeofarrivals = []\n",
    "    langs = []\n",
    "\n",
    "    # xmlを解析し、rootを取得\n",
    "    tree = et.parse(f)\n",
    "    elem = tree.getroot()\n",
    "    # エラーとなった記事をカウントする変数を初期化\n",
    "    fail_cnt = 0\n",
    "\n",
    "    # 解析したxmlから、記事単位で要素を取得\n",
    "    contents = elem.getiterator('ContentT')\n",
    "\n",
    "    for content in contents:\n",
    "        try:\n",
    "            # 記事の言語情報を取得し、日本語か英語の記事であれば以降の処理を実施\n",
    "            lang = content.find(\".//LanguageString\").text\n",
    "            if lang == 'JAPANESE' or lang == 'ENGLISH':\n",
    "                # 言語、ID、タイトル、タイムスタンプを配列に格納\n",
    "                langs.append(lang)\n",
    "                ids.append(content.find(\".//Id/SUID\").text)\n",
    "                headlines.append(content.find(\".//Headline\").text)\n",
    "                timeofarrivals.append(content.find(\".//TimeOfArrival\").text)\n",
    "        except:\n",
    "            # 読み取りに失敗した場合はカウント\n",
    "            fail_cnt += 1\n",
    "\n",
    "    # カラムに対応する配列を用いてDataFrameを作成\n",
    "    df = DataFrame({\n",
    "            \"Id\":ids,\n",
    "            \"Headline\":headlines,\n",
    "            \"TimeOfArrival\":timeofarrivals,\n",
    "            \"Language\": langs\n",
    "        }).drop_duplicates() # 重複レコードを削除\n",
    "\n",
    "    # DataFrameをCSVファイルとして保存\n",
    "    df.to_csv(os.path.join(DATA_DIR,'csv1',file_name.replace(\".xml.gz\",\".csv\")),encoding='utf8',header=False,index=False)\n",
    "    f.close()\n",
    "\n",
    "    print('file_name:' + file_name + ' time:' + str(datetime.datetime.now()-start) + ' record_count:' + str(len(df)) + ' fail_count:' + str(fail_cnt))\n",
    "\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "print('This processing started at ' + str(start_time))\n",
    "\n",
    "# dataフォルダ配下の全ファイルを取得\n",
    "files = os.listdir(DATA_DIR+'/original_Data')\n",
    "# csvフォルダ配下の全フォルダを取得\n",
    "csvs = os.listdir(DATA_DIR + '/csv1/')\n",
    "\n",
    "for i, gz in enumerate(files):\n",
    "    # ファイルの拡張子がxml.gzで、まだcsvファイルが存在しない場合は処理を実施\n",
    "    if gz[-6:] == 'xml.gz' and gz[:17]+'.csv' not in csvs:\n",
    "        convertToCSV(gz)\n",
    "\n",
    "print('It taked ' + str(datetime.datetime.now()) + ' seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ナカシマのcsvの処理。bodyが入っていて、日本語しか扱っていない。csv2に格納\n",
    "def my_convertToCSV(file_name):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # 引数として渡されたtar.gzファイルを解凍し、オープン\n",
    "    f = gzip.open(os.path.join(DATA_DIR,'EID42186/original_Data',file_name), 'r')\n",
    "\n",
    "    # CSVファイルのカラムに対応する配列を初期化\n",
    "    ids = []\n",
    "    headlines = []\n",
    "    timeofarrivals = []\n",
    "    bodys = []\n",
    "    #langs = []\n",
    "\n",
    "    # xmlを解析し、rootを取得\n",
    "    tree = et.parse(f)\n",
    "    elem = tree.getroot()\n",
    "    # エラーとなった記事をカウントする変数を初期化\n",
    "    fail_cnt = 0\n",
    "\n",
    "    # 解析したxmlから、記事単位で要素を取得\n",
    "    contents = elem.getiterator('ContentT')\n",
    "\n",
    "    for content in contents:\n",
    "        try:\n",
    "            # 記事の言語情報を取得し、日本語か英語の記事であれば以降の処理を実施\n",
    "            lang = content.find(\".//LanguageString\").text\n",
    "            #if lang == 'JAPANESE' or lang == 'ENGLISH':\n",
    "            if lang == 'JAPANESE':\n",
    "                if content.find(\".//Body\").text is not ' ':\n",
    "                    # 言語、ID、タイトル、タイムスタンプを配列に格納\n",
    "                    #langs.append(lang)\n",
    "                    ids.append(content.find(\".//Id/SUID\").text)\n",
    "                    headlines.append(content.find(\".//Headline\").text)\n",
    "                    timeofarrivals.append(content.find(\".//TimeOfArrival\").text)\n",
    "                    bodys.append(content.find(\".//Body\").text)\n",
    "        except:\n",
    "            # 読み取りに失敗した場合はカウント\n",
    "            fail_cnt += 1\n",
    "            #traceback.print_exc()\n",
    "    \n",
    "    print('fail_cnt:', fail_cnt)\n",
    "    \n",
    "    # カラムに対応する配列を用いてDataFrameを作成\n",
    "    df = DataFrame({\"Id\":ids})\n",
    "    df['Headline']=headlines\n",
    "    df['Body']=bodys\n",
    "    df['TimeOfArrival']=timeofarrivals\n",
    "    df = df.drop_duplicates(['Id'])\n",
    "    # DataFrameをCSVファイルとして保存\n",
    "    df.to_csv(os.path.join(DATA_DIR,'EID42186/myCsv',file_name.replace(\".xml.gz\",\".csv\")),encoding='utf8',index=False)\n",
    "    f.close()\n",
    "\n",
    "    print('file_name:' + file_name + ' time:' + str(datetime.datetime.now()-start) + ' record_count:' + str(len(df)) + ' fail_count:' + str(fail_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#　全ファイル取得\n",
    "files = os.listdir(DATA_DIR+'/EID42186/original_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataフォルダ配下の全ファイルを取得\n",
    "for i in range(0,len(files)):\n",
    "    my_convertToCSV(files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files[4])\n",
    "file_name = files[4]\n",
    "#hoge = pd.read_csv(os.path.join(DATA_DIR,'EID42186/csv/2014',file_name.replace(\".xml.gz\",\".csv\")))\n",
    "hoge = pd.read_csv(os.path.join(DATA_DIR,'EID42186/myCsv',file_name.replace(\".xml.gz\",\".csv\")), )#.drop(['Id', 'TimeOfArrival'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoge.drop_duplicates('Id').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[(df[2]=='JAPANESE') & (df[0].str.contains('：'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data解析　実験\n",
    "from janome.tokenizer import Tokenizer\n",
    "file_name = files[1]\n",
    "hoge = pd.read_csv(os.path.join(DATA_DIR,'csvData1',file_name.replace(\".xml.gz\",\".csv\")))\n",
    "headlines = hoge['Headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使い方リマインド\n",
    "t = Tokenizer()\n",
    "#tokens = t.tokenize(headlines[0])\n",
    "tokens = t.tokenize('安倍晋三首相と、麻生太郎副総理兼財務相が、新たな「密約」を結んだという情報が飛び込んできた。中島悠太郎とドナルド・トランプは眠い')\n",
    "for token in tokens:\n",
    "    #print(token)\n",
    "    #if (token.part_of_speech.split(',')[0]=='名詞'):\n",
    "    if (token.part_of_speech.split(',')[2]=='人名'):\n",
    "        print(token)\n",
    "        #and token.part_of_speech.split(',')[1]=='固有名詞'):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = files[1]\n",
    "hoge = pd.read_csv(os.path.join(DATA_DIR,'csvData1',file_name.replace(\".xml.gz\",\".csv\"))).drop(['Id', 'Body', 'TimeOfArrival'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoge.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hoge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "for i in range(0,hoge.shape[0]):\n",
    "    tokens = t.tokenize(hoge['Headline'][i])\n",
    "    for token in tokens:\n",
    "        #print(token)\n",
    "        #if (token.part_of_speech.split(',')[0]=='名詞'):\n",
    "        if (token.part_of_speech.split(',')[2]=='人名'):\n",
    "            print(token)\n",
    "            #and token.part_of_speech.split(',')[1]=='固有名詞'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(hoge.stack().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# just for measuring time\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# need to recieve unicode text, this is callable for TfidfVectorizer\n",
    "# need to recieve unicode text\n",
    "def myTokenizer(text):\n",
    "    #TARGET_CATEGORY = [\"名詞\", \"動詞\",  \"形容詞\", \"副詞\", \"連体詞\", \"助動詞\"]\n",
    "    #wordsIn=[]\n",
    "    #t = Tokenizer()\n",
    "    #tokens = t.tokenize(text)\n",
    "    #for token in tokens:\n",
    "    #    tokenCategory = token.part_of_speech.split(',')[0]\n",
    "    #    tokenBasic = token.base_form\n",
    "    #    if  (tokenCategory=='名詞' and token.part_of_speech.split(',')[1]=='固有名詞'):\n",
    "    #        wordsIn.append(token.surface)\n",
    "    #    elif tokenCategory in TARGET_CATEGORY:\n",
    "    #        if tokenBasic != '*':                               #if basic form can be defined\n",
    "    #            wordsIn.append(tokenBasic)\n",
    "    wordsIn=[]\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(text)\n",
    "    for token in tokens:\n",
    "        #print(token)\n",
    "        #if (token.part_of_speech.split(',')[0]=='名詞'):\n",
    "        if (token.part_of_speech.split(',')[2]=='人名'):\n",
    "            wordsIn.append(token.surface)\n",
    "    return wordsIn\n",
    "\n",
    "\n",
    "#vectorizer = CountVectorizer(ngram_range=(1, 2),tokenizer=myTokenizer)     \n",
    "#tfidf_weighted_matrix = vectorizer.fit_transform(tweetsProcessed)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2),tokenizer=myTokenizer,min_df=2).fit(hoge.stack().tolist()) # stop_words = ''\n",
    "bow = vectorizer.transform(hoge.stack().tolist())\n",
    "\n",
    "# for time\n",
    "elapsed_time = time.time() - start\n",
    "print(elapsed_time)\n",
    "print(\"bag_of_words with df as 2: {}\\n\".format(repr(bow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_value = bow.max(axis=0).toarray().ravel()\n",
    "sorted_by_num = max_value.argsort()\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print(\"Features with highest tfidf: \\n{}\\n\".format(\n",
    "      feature_names[sorted_by_num[-30:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 30 features:\\n{}\".format(vectorizer.vocabulary_))?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freqs = [(word, bow.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "print (sorted (freqs, key = lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://labs.goo.ne.jp/api/jp/named-entity-extraction/\n",
    "# うまくいかないようであれば"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 図示にあたって"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime.strptime('20140101', '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = makeDateList(start, '7')\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = datetime.datetime.strptime('20140201', '%Y%m%d')\n",
    "b = datetime.datetime.strptime('20140301', '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a<b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# date:headlineのdictを受け取る\n",
    "# flagは、月=(month)か年(=year)か\n",
    "def shapeData(data, flag):\n",
    "    res = pd.DataFrame()\n",
    "    if (flag=='month'):\n",
    "        \n",
    "    elif(flag=='year'):\n",
    "        \n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
