{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip,os,tarfile,sys\n",
    "sys.path.append(os.pardir+'/src')\n",
    "from settings import *\n",
    "from boto3.session import Session\n",
    "import datetime\n",
    "import traceback\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import xml.etree.ElementTree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# S3から記事データをダウンロードする関数\n",
    "def downloadFile(bucket, tag, target_day):\n",
    "    # バケットから指定されたタグと日付に該当するオブジェクトを取得\n",
    "    objects = bucket.objects.all().filter(Prefix=tag+target_day)\n",
    "\n",
    "    for object in objects:\n",
    "        # データを格納するパスを生成\n",
    "        path = os.path.join(DATA_DIR,tag+target_day)\n",
    "        # ダウンロードを実施\n",
    "        bucket.download_file(object.key, path)\n",
    "\n",
    "# startで指定された日付からspan日分のファイル名配列を生成\n",
    "def makeDateList(start, span):\n",
    "    dateList = []\n",
    "\n",
    "    for i in range(int(span)):\n",
    "        dateList.append('EID34151_' + start.strftime(\"%Y%m%d\") + '.xml.gz')\n",
    "        start = start + datetime.timedelta(days=1)\n",
    "    return dateList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# コマンドライン引数からダウンロードを開始する日付と範囲を取得\n",
    "start_date = '20130101'\n",
    "span = '365'\n",
    "start = datetime.datetime.strptime(start_date, '%Y%m%d')\n",
    "\n",
    "# S3へ接続\n",
    "session = Session(aws_access_key_id=AWS_ACCESS_KEY_ID,aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n",
    "s3 = session.resource('s3')\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "\n",
    "# tagと日付リストを設定\n",
    "tag = \"EID34151_\"\n",
    "dateList = makeDateList(start, span)\n",
    "\n",
    "for date in dateList:\n",
    "    time = datetime.datetime.now()\n",
    "    date = date[9:]\n",
    "    try:\n",
    "        downloadFile(bucket, tag, date)\n",
    "        print(date + ' was done ' + str(datetime.datetime.now()-time))\n",
    "    except Exception as e:\n",
    "        print('error! ' + date)\n",
    "        print(logging.error(traceback.format_exc()))\n",
    "\n",
    "print('it taked ' + str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 片山さんのプログラムによる抽出 csv1へ格納\n",
    "import gzip,os,tarfile,sys\n",
    "from settings import *\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from boto3.session import Session\n",
    "import xml.etree.ElementTree as et\n",
    "import datetime\n",
    "\n",
    "\n",
    "# xml.gzファイルを解凍、xmlの構造を解析しCSVファイルを作成する関数\n",
    "def convertToCSV(file_name):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # 引数として渡されたtar.gzファイルを解凍し、オープン\n",
    "    f = gzip.open(os.path.join(DATA_DIR,'original_Data',file_name), 'r')\n",
    "\n",
    "    # CSVファイルのカラムに対応する配列を初期化\n",
    "    ids = []\n",
    "    headlines = []\n",
    "    timeofarrivals = []\n",
    "    langs = []\n",
    "\n",
    "    # xmlを解析し、rootを取得\n",
    "    tree = et.parse(f)\n",
    "    elem = tree.getroot()\n",
    "    # エラーとなった記事をカウントする変数を初期化\n",
    "    fail_cnt = 0\n",
    "\n",
    "    # 解析したxmlから、記事単位で要素を取得\n",
    "    contents = elem.getiterator('ContentT')\n",
    "\n",
    "    for content in contents:\n",
    "        try:\n",
    "            # 記事の言語情報を取得し、日本語か英語の記事であれば以降の処理を実施\n",
    "            lang = content.find(\".//LanguageString\").text\n",
    "            if lang == 'JAPANESE' or lang == 'ENGLISH':\n",
    "                # 言語、ID、タイトル、タイムスタンプを配列に格納\n",
    "                langs.append(lang)\n",
    "                ids.append(content.find(\".//Id/SUID\").text)\n",
    "                headlines.append(content.find(\".//Headline\").text)\n",
    "                timeofarrivals.append(content.find(\".//TimeOfArrival\").text)\n",
    "        except:\n",
    "            # 読み取りに失敗した場合はカウント\n",
    "            fail_cnt += 1\n",
    "\n",
    "    # カラムに対応する配列を用いてDataFrameを作成\n",
    "    df = DataFrame({\n",
    "            \"Id\":ids,\n",
    "            \"Headline\":headlines,\n",
    "            \"TimeOfArrival\":timeofarrivals,\n",
    "            \"Language\": langs\n",
    "        }).drop_duplicates() # 重複レコードを削除\n",
    "\n",
    "    # DataFrameをCSVファイルとして保存\n",
    "    df.to_csv(os.path.join(DATA_DIR,'csv1',file_name.replace(\".xml.gz\",\".csv\")),encoding='utf8',header=False,index=False)\n",
    "    f.close()\n",
    "\n",
    "    print('file_name:' + file_name + ' time:' + str(datetime.datetime.now()-start) + ' record_count:' + str(len(df)) + ' fail_count:' + str(fail_cnt))\n",
    "\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "print('This processing started at ' + str(start_time))\n",
    "\n",
    "# dataフォルダ配下の全ファイルを取得\n",
    "files = os.listdir(DATA_DIR+'/original_Data')\n",
    "# csvフォルダ配下の全フォルダを取得\n",
    "csvs = os.listdir(DATA_DIR + '/csv1/')\n",
    "\n",
    "for i, gz in enumerate(files):\n",
    "    # ファイルの拡張子がxml.gzで、まだcsvファイルが存在しない場合は処理を実施\n",
    "    if gz[-6:] == 'xml.gz' and gz[:17]+'.csv' not in csvs:\n",
    "        convertToCSV(gz)\n",
    "\n",
    "print('It taked ' + str(datetime.datetime.now()) + ' seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ナカシマのcsvの処理。bodyが入っていて、日本語しか扱っていない。csv2に格納\n",
    "def my_convertToCSV(file_name):\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    # 引数として渡されたtar.gzファイルを解凍し、オープン\n",
    "    f = gzip.open(os.path.join(DATA_DIR,'EID34151/original_Data/2017',file_name), 'r')\n",
    "\n",
    "    # CSVファイルのカラムに対応する配列を初期化\n",
    "    ids = []\n",
    "    headlines = []\n",
    "    timeofarrivals = []\n",
    "    bodys = []\n",
    "    #langs = []\n",
    "\n",
    "    # xmlを解析し、rootを取得\n",
    "    tree = et.parse(f)\n",
    "    elem = tree.getroot()\n",
    "    # エラーとなった記事をカウントする変数を初期化\n",
    "    fail_cnt = 0\n",
    "\n",
    "    # 解析したxmlから、記事単位で要素を取得\n",
    "    contents = elem.getiterator('ContentT')\n",
    "\n",
    "    for content in contents:\n",
    "        try:\n",
    "            # 記事の言語情報を取得し、日本語か英語の記事であれば以降の処理を実施\n",
    "            lang = content.find(\".//LanguageString\").text\n",
    "            #if lang == 'JAPANESE' or lang == 'ENGLISH':\n",
    "            if lang == 'JAPANESE':\n",
    "                if content.find(\".//Body\").text is not ' ':\n",
    "                    # 言語、ID、タイトル、タイムスタンプを配列に格納\n",
    "                    #langs.append(lang)\n",
    "                    ids.append(content.find(\".//Id/SUID\").text)\n",
    "                    headlines.append(content.find(\".//Headline\").text)\n",
    "                    timeofarrivals.append(content.find(\".//TimeOfArrival\").text)\n",
    "                    bodys.append(content.find(\".//Body\").text)\n",
    "        except:\n",
    "            # 読み取りに失敗した場合はカウント\n",
    "            fail_cnt += 1\n",
    "            #traceback.print_exc()\n",
    "    \n",
    "    print('fail_cnt:', fail_cnt)\n",
    "    \n",
    "    # カラムに対応する配列を用いてDataFrameを作成\n",
    "    df = DataFrame({\"Id\":ids})\n",
    "    df['Headline']=headlines\n",
    "    df['Body']=bodys\n",
    "    df['TimeOfArrival']=timeofarrivals\n",
    "    df = df.drop_duplicates(['Id'])\n",
    "    # DataFrameをCSVファイルとして保存\n",
    "    df.to_csv(os.path.join(DATA_DIR,'EID34151/myCsv',file_name.replace(\".xml.gz\",\".csv\")),encoding='utf8',index=False)\n",
    "    f.close()\n",
    "\n",
    "    print('file_name:' + file_name + ' time:' + str(datetime.datetime.now()-start) + ' record_count:' + str(len(df)) + ' fail_count:' + str(fail_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#　全ファイル取得\n",
    "files_2014 = os.listdir(DATA_DIR+'/EID34151/original_Data/2014')\n",
    "files_2015 = os.listdir(DATA_DIR+'/EID34151/original_Data/2015')\n",
    "files_2016 = os.listdir(DATA_DIR+'/EID34151/original_Data/2016')\n",
    "files_2017 = os.listdir(DATA_DIR+'/EID34151/original_Data/2017')\n",
    "\n",
    "hoge = [files_2015, files_2016, files_2017]\n",
    "\n",
    "# EID34151_20150203.xml.gzが展開できず\n",
    "# EID34151_20151026.xml.gzが展開できず"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EID34151_20170113.xml.gz'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_2017[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_2016.remove('EID34151_20161216.xml.gz')\n",
    "files_2016.remove('EID34151_20161215.xml.gz')\n",
    "files_2016.remove('EID34151_20160428.xml.gz')\n",
    "files_2016.remove('EID34151_20160524.xml.gz')\n",
    "files_2016.remove('EID34151_20160602.xml.gz')\n",
    "files_2016.remove('EID34151_20160607.xml.gz')\n",
    "files_2016.remove('EID34151_20160211.xml.gz')\n",
    "files_2016.remove('EID34151_20160301.xml.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail_cnt: 62\n",
      "file_name:EID34151_20170101.xml.gz time:0:00:12.992308 record_count:679 fail_count:62\n",
      "fail_cnt: 60\n",
      "file_name:EID34151_20170102.xml.gz time:0:00:34.181156 record_count:172 fail_count:60\n",
      "fail_cnt: 402\n",
      "file_name:EID34151_20170103.xml.gz time:0:01:39.915972 record_count:834 fail_count:402\n",
      "fail_cnt: 873\n",
      "file_name:EID34151_20170104.xml.gz time:0:01:46.678015 record_count:1554 fail_count:873\n",
      "fail_cnt: 1347\n",
      "file_name:EID34151_20170105.xml.gz time:0:01:53.983500 record_count:1119 fail_count:1347\n",
      "fail_cnt: 1412\n",
      "file_name:EID34151_20170106.xml.gz time:0:01:48.867080 record_count:876 fail_count:1412\n",
      "fail_cnt: 893\n",
      "file_name:EID34151_20170107.xml.gz time:0:00:18.803075 record_count:51 fail_count:893\n",
      "fail_cnt: 72\n",
      "file_name:EID34151_20170108.xml.gz time:0:00:37.313134 record_count:509 fail_count:72\n",
      "fail_cnt: 464\n",
      "file_name:EID34151_20170109.xml.gz time:0:02:18.626374 record_count:1158 fail_count:464\n",
      "fail_cnt: 905\n",
      "file_name:EID34151_20170110.xml.gz time:0:02:10.804679 record_count:915 fail_count:905\n",
      "fail_cnt: 1727\n",
      "file_name:EID34151_20170111.xml.gz time:0:02:44.024647 record_count:1098 fail_count:1727\n",
      "fail_cnt: 1749\n",
      "file_name:EID34151_20170112.xml.gz time:0:02:26.540772 record_count:1100 fail_count:1749\n",
      "error\n",
      "EID34151_20170113.xml.gz\n",
      "fail_cnt: 684\n",
      "file_name:EID34151_20170114.xml.gz time:0:00:25.695470 record_count:70 fail_count:684\n",
      "fail_cnt: 186\n",
      "file_name:EID34151_20170115.xml.gz time:0:00:41.514375 record_count:986 fail_count:186\n",
      "fail_cnt: 745\n",
      "file_name:EID34151_20170116.xml.gz time:0:01:40.852077 record_count:1033 fail_count:745\n",
      "fail_cnt: 1582\n",
      "file_name:EID34151_20170117.xml.gz time:0:02:10.578603 record_count:810 fail_count:1582\n",
      "fail_cnt: 1606\n",
      "file_name:EID34151_20170118.xml.gz time:0:02:31.636282 record_count:1003 fail_count:1606\n",
      "fail_cnt: 1486\n",
      "file_name:EID34151_20170119.xml.gz time:0:02:46.962663 record_count:1299 fail_count:1486\n",
      "fail_cnt: 1511\n",
      "file_name:EID34151_20170120.xml.gz time:0:01:52.486143 record_count:947 fail_count:1511\n",
      "fail_cnt: 736\n",
      "file_name:EID34151_20170121.xml.gz time:0:00:19.142696 record_count:85 fail_count:736\n",
      "fail_cnt: 160\n",
      "file_name:EID34151_20170122.xml.gz time:0:00:43.205007 record_count:968 fail_count:160\n",
      "fail_cnt: 790\n",
      "file_name:EID34151_20170123.xml.gz time:0:02:06.611697 record_count:3241 fail_count:790\n",
      "fail_cnt: 1261\n",
      "file_name:EID34151_20170124.xml.gz time:0:02:23.477420 record_count:1046 fail_count:1261\n",
      "fail_cnt: 1636\n",
      "file_name:EID34151_20170125.xml.gz time:0:02:21.955164 record_count:1154 fail_count:1636\n",
      "fail_cnt: 1723\n",
      "file_name:EID34151_20170126.xml.gz time:0:02:10.199155 record_count:1366 fail_count:1723\n",
      "fail_cnt: 1583\n",
      "file_name:EID34151_20170127.xml.gz time:0:01:58.860040 record_count:1806 fail_count:1583\n",
      "fail_cnt: 880\n",
      "file_name:EID34151_20170128.xml.gz time:0:00:25.912423 record_count:69 fail_count:880\n",
      "fail_cnt: 138\n",
      "file_name:EID34151_20170129.xml.gz time:0:00:40.264513 record_count:1126 fail_count:138\n",
      "fail_cnt: 1281\n",
      "file_name:EID34151_20170130.xml.gz time:0:01:55.448658 record_count:1773 fail_count:1281\n"
     ]
    }
   ],
   "source": [
    "# dataフォルダ配下の全ファイルを取得\n",
    "for i in range(0,len(files_2017)):\n",
    "    try:\n",
    "        my_convertToCSV(files_2017[i])\n",
    "    except:\n",
    "        print('error')\n",
    "        print(files_2017[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "not well-formed (invalid token): line 41236769, column 0 (<string>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<string>\"\u001b[1;36m, line \u001b[1;32munknown\u001b[0m\n\u001b[1;31mParseError\u001b[0m\u001b[1;31m:\u001b[0m not well-formed (invalid token): line 41236769, column 0\n"
     ]
    }
   ],
   "source": [
    "my_convertToCSV(files_2017[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "以下実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data解析　実験\n",
    "from janome.tokenizer import Tokenizer\n",
    "file_name = files[1]\n",
    "hoge = pd.read_csv(os.path.join(DATA_DIR,'csvData1',file_name.replace(\".xml.gz\",\".csv\")))\n",
    "headlines = hoge['Headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 使い方リマインド\n",
    "t = Tokenizer()\n",
    "#tokens = t.tokenize(headlines[0])\n",
    "tokens = t.tokenize('安倍晋三首相と、麻生太郎副総理兼財務相が、新たな「密約」を結んだという情報が飛び込んできた。中島悠太郎とドナルド・トランプは眠い')\n",
    "for token in tokens:\n",
    "    #print(token)\n",
    "    #if (token.part_of_speech.split(',')[0]=='名詞'):\n",
    "    if (token.part_of_speech.split(',')[2]=='人名'):\n",
    "        print(token)\n",
    "        #and token.part_of_speech.split(',')[1]=='固有名詞'):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = files[1]\n",
    "hoge = pd.read_csv(os.path.join(DATA_DIR,'csvData1',file_name.replace(\".xml.gz\",\".csv\"))).drop(['Id', 'Body', 'TimeOfArrival'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hoge.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hoge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "for i in range(0,hoge.shape[0]):\n",
    "    tokens = t.tokenize(hoge['Headline'][i])\n",
    "    for token in tokens:\n",
    "        #print(token)\n",
    "        #if (token.part_of_speech.split(',')[0]=='名詞'):\n",
    "        if (token.part_of_speech.split(',')[2]=='人名'):\n",
    "            print(token)\n",
    "            #and token.part_of_speech.split(',')[1]=='固有名詞'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(hoge.stack().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# just for measuring time\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# need to recieve unicode text, this is callable for TfidfVectorizer\n",
    "# need to recieve unicode text\n",
    "def myTokenizer(text):\n",
    "    #TARGET_CATEGORY = [\"名詞\", \"動詞\",  \"形容詞\", \"副詞\", \"連体詞\", \"助動詞\"]\n",
    "    #wordsIn=[]\n",
    "    #t = Tokenizer()\n",
    "    #tokens = t.tokenize(text)\n",
    "    #for token in tokens:\n",
    "    #    tokenCategory = token.part_of_speech.split(',')[0]\n",
    "    #    tokenBasic = token.base_form\n",
    "    #    if  (tokenCategory=='名詞' and token.part_of_speech.split(',')[1]=='固有名詞'):\n",
    "    #        wordsIn.append(token.surface)\n",
    "    #    elif tokenCategory in TARGET_CATEGORY:\n",
    "    #        if tokenBasic != '*':                               #if basic form can be defined\n",
    "    #            wordsIn.append(tokenBasic)\n",
    "    wordsIn=[]\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(text)\n",
    "    for token in tokens:\n",
    "        #print(token)\n",
    "        #if (token.part_of_speech.split(',')[0]=='名詞'):\n",
    "        if (token.part_of_speech.split(',')[2]=='人名'):\n",
    "            wordsIn.append(token.surface)\n",
    "    return wordsIn\n",
    "\n",
    "\n",
    "#vectorizer = CountVectorizer(ngram_range=(1, 2),tokenizer=myTokenizer)     \n",
    "#tfidf_weighted_matrix = vectorizer.fit_transform(tweetsProcessed)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2),tokenizer=myTokenizer,min_df=2).fit(hoge.stack().tolist()) # stop_words = ''\n",
    "bow = vectorizer.transform(hoge.stack().tolist())\n",
    "\n",
    "# for time\n",
    "elapsed_time = time.time() - start\n",
    "print(elapsed_time)\n",
    "print(\"bag_of_words with df as 2: {}\\n\".format(repr(bow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_value = bow.max(axis=0).toarray().ravel()\n",
    "sorted_by_num = max_value.argsort()\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print(\"Features with highest tfidf: \\n{}\\n\".format(\n",
    "      feature_names[sorted_by_num[-30:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"First 30 features:\\n{}\".format(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freqs = [(word, bow.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "print (sorted (freqs, key = lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://labs.goo.ne.jp/api/jp/named-entity-extraction/\n",
    "# うまくいかないようであれば"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 図示にあたって"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime.strptime('20140101', '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = makeDateList(start, '7')\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = datetime.datetime.strptime('20140201', '%Y%m%d')\n",
    "b = datetime.datetime.strptime('20140301', '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a<b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# date:headlineのdictを受け取る\n",
    "# flagは、月=(month)か年(=year)か\n",
    "def shapeData(data, flag):\n",
    "    res = pd.DataFrame()\n",
    "    if (flag=='month'):\n",
    "        \n",
    "    elif(flag=='year'):\n",
    "        \n",
    "    else:\n",
    "        raise Exception\n",
    "    \n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
